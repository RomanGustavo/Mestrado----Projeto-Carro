{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomanGustavo/Mestrado----Projeto-Carro/blob/main/TWICE_Colab_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905f97d3",
      "metadata": {
        "id": "905f97d3"
      },
      "source": [
        "# TWICE Dataset — Colab Starter\n",
        "Este notebook prepara o ambiente, baixa o **subset** do TWICE, lê `.osi`, projeta cuboids (via utilitários do repo) e mostra como visualizar radar e rodar um detector (YOLOv7) para DEMO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 1 — instalar pacotes\n",
        "\n",
        "o que faz: pip install de dependências usadas ao longo do notebook:\n",
        "\n",
        "opencv-python (ler/decodificar imagens JPEG dentro dos .osi, fazer vídeo),\n",
        "\n",
        "tqdm (barrinhas de progresso),\n",
        "\n",
        "open3d (visualizar/grav ar nuvem de pontos .ply — útil pro LiDAR),\n",
        "\n",
        "protobuf==4.25.3 (compatível com as mensagens OSI),\n",
        "\n",
        "matplotlib (gráficos simples).\n",
        "\n",
        "relação com o TWICE: os arquivos do TWICE usam OSI via protobuf; sem o protobuf correto e o OpenCV, você não abre os frames da câmera nem gera os vídeos."
      ],
      "metadata": {
        "id": "i7nu_XHJfPPy"
      },
      "id": "i7nu_XHJfPPy"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "acb83af7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "acb83af7",
        "outputId": "f9534b9f-0e11-4889-8d8d-1c8f08f0291b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==4.25.3\n",
            "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.3\n",
            "    Uninstalling protobuf-4.25.3:\n",
            "      Successfully uninstalled protobuf-4.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "e553b75ef0a74f9f91495f76d8c6621c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grpcio-status==1.60.0 in /usr/local/lib/python3.12/dist-packages (1.60.0)\n",
            "Requirement already satisfied: protobuf>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from grpcio-status==1.60.0) (4.25.3)\n",
            "Requirement already satisfied: grpcio>=1.60.0 in /usr/local/lib/python3.12/dist-packages (from grpcio-status==1.60.0) (1.74.0)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.5.5 in /usr/local/lib/python3.12/dist-packages (from grpcio-status==1.60.0) (1.70.0)\n"
          ]
        }
      ],
      "source": [
        "# CEL 1 — pacotes base\n",
        "!pip -q install --upgrade opencv-python tqdm open3d protobuf==4.25.3 matplotlib\n",
        "\n",
        "# Forçar protobuf 4 só para os scripts OSI/TWICE\n",
        "!pip install \"protobuf==4.25.3\" --force-reinstall\n",
        "\n",
        "# Depois reinstalar o grpcio-status na versão compatível com protobuf 4\n",
        "!pip install \"grpcio-status==1.60.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 2 — instalar/ativar o OSI (Open Simulation Interface)\n",
        "\n",
        "o que faz: tenta import osi3. se não existir, clona o repositório oficial do OSI, gera os módulos python a partir dos .proto (com setup.py build_py) e instala com pip.\n",
        "\n",
        "relação com o TWICE: os .osi do TWICE são mensagens protobuf do OSI (classes como SensorData e SensorDataSeries). sem o pacote osi3, você não consegue decodificar o conteúdo dos arquivos de câmera/radar/lidar/IMU."
      ],
      "metadata": {
        "id": "CWU6Ug0Oe-60"
      },
      "id": "CWU6Ug0Oe-60"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e0637233",
      "metadata": {
        "id": "e0637233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd37edd-eb9c-456d-b42e-41fd9cdb3064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ osi3 já disponível\n"
          ]
        }
      ],
      "source": [
        "# CEL 2 — Open Simulation Interface (OSI): tenta importar; se não existir, compila a partir do repo oficial\n",
        "try:\n",
        "    import osi3\n",
        "    print(\"✅ osi3 já disponível\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ osi3 não encontrado. Clonando OSI e instalando a partir do código-fonte...\")\n",
        "    !git clone -q https://github.com/OpenSimulationInterface/open-simulation-interface.git\n",
        "    %cd /content/open-simulation-interface\n",
        "    !python3 setup.py -q build_py\n",
        "    !pip -q install .\n",
        "    %cd /content\n",
        "    import osi3\n",
        "    print(\"✅ osi3 instalado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 3 — clonar os scripts do repositório TWICE\n",
        "\n",
        "o que faz: git clone de TWICEdataset/scripts e entra na pasta.\n",
        "\n",
        "relação com o TWICE: esse repo traz utilitários prontos que já entendem a estrutura OSI/OpenLABEL do TWICE:\n",
        "\n",
        "cuboid_project.py: projeção de cuboids 3D → imagem (usa intrínsecas/extrínsecas).\n",
        "\n",
        "radar_osi_plot.py: leitura/plot do radar (modos cluster e object list) + trajetória.\n",
        "\n",
        "lidar_osi2PLY.py: converter LiDAR .osi → .ply.\n",
        "\n",
        "tutorial.ipynb: exemplos prontos."
      ],
      "metadata": {
        "id": "Ek8fyQGZgYqw"
      },
      "id": "Ek8fyQGZgYqw"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fde90579",
      "metadata": {
        "id": "fde90579",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea90f49-d9fb-4306-d813-ad1b3ddb9c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/scripts\n",
            "total 136\n",
            "drwxr-xr-x 6 root root  4096 Aug 24 14:56 .\n",
            "drwxr-xr-x 1 root root  4096 Aug 24 14:15 ..\n",
            "-rw-r--r-- 1 root root 16496 Aug 24 14:15 cuboid_project.py\n",
            "-rw-r--r-- 1 root root  4891 Aug 24 14:15 file_selector.py\n",
            "drwxr-xr-x 8 root root  4096 Aug 24 14:15 .git\n",
            "-rw-r--r-- 1 root root  3317 Aug 24 14:15 lidar_osi2PLY.py\n",
            "drwxr-xr-x 2 root root  4096 Aug 24 14:15 __pycache__\n",
            "-rw-r--r-- 1 root root 27662 Aug 24 14:15 radar_osi_plot.py\n",
            "-rw-r--r-- 1 root root  1545 Aug 24 14:15 README.md\n",
            "-rw-r--r-- 1 root root   117 Aug 24 14:15 requirements.txt\n",
            "drwxr-xr-x 2 root root  4096 Aug 24 14:15 Results\n",
            "drwxr-xr-x 5 root root  4096 Aug 24 14:56 scripts\n",
            "-rw-r--r-- 1 root root 18228 Aug 24 14:15 tutorial.ipynb\n",
            "-rw-r--r-- 1 root root 15554 Aug 24 14:15 TWICE_df.csv\n",
            "-rw-r--r-- 1 root root    20 Aug 24 14:15 TWICE_path.txt\n",
            "-rw-r--r-- 1 root root  2350 Aug 24 14:15 vehicle_to_image.py\n"
          ]
        }
      ],
      "source": [
        "# CEL 3 — clonar scripts\n",
        "!git clone -q https://github.com/TWICEdataset/scripts.git\n",
        "%cd /content/scripts\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 4 — baixar e extrair o subset do dataset\n",
        "\n",
        "o que faz: baixa TWICEsubset.zip (~439 MB) do servidor e extrai em /content/TWICE.\n",
        "\n",
        "CEL 4 “robutizada”\n",
        "\n",
        "urllib + certifi (mantém validação TLS)\n",
        "\n",
        "wget com --no-check-certificate (ignora verificação se 1 falhar)\n",
        "\n",
        "curl -k (fallback final, também ignora verificação)\n",
        "\n",
        "Depois ela verifica o arquivo, mostra tamanho e extrai\n",
        "\n",
        "relação com o TWICE: você passa a ter localmente a árvore de cenários do dataset (ex.: Scenarios/dynamic ego/.../real/daytime/.../{Camera,Radar,LiDAR,IMU ...}), com arquivos .osi por sensor e arquivos OpenLABEL .json (anotações e calibrações)."
      ],
      "metadata": {
        "id": "5oLcm_2rgsIG"
      },
      "id": "5oLcm_2rgsIG"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "878f3158",
      "metadata": {
        "id": "878f3158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3783650-bda5-4f4d-922f-70303cd9acc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Tentando baixar com urllib + certifi (TLS verificado)...\n",
            "❌ Falhou urllib+certifi: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)>\n",
            "⬇️ Tentando com wget (sem checar certificado)...\n",
            "✅ Baixado com wget.\n",
            "📏 Tamanho do ZIP: 438.5 MB\n",
            "🔐 SHA256: b3469614e3e95491 …\n",
            "📦 Extraindo…\n",
            "✅ Pronto. Dados em: /content/TWICE\n"
          ]
        }
      ],
      "source": [
        "# CEL 4 (robusta) — baixar o subset e extrair, contornando SSL do servidor\n",
        "import os, zipfile, pathlib, hashlib, sys\n",
        "\n",
        "subset_url = \"https://twice.eletrica.ufpr.br/TWICEsubset.zip\"\n",
        "zip_path = \"/content/TWICEsubset.zip\"\n",
        "out_dir = pathlib.Path(\"/content/TWICE\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def sha256sum(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "ok = False\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"⬇️ Tentando baixar com urllib + certifi (TLS verificado)...\")\n",
        "    try:\n",
        "        import ssl, certifi, urllib.request\n",
        "        ctx = ssl.create_default_context(cafile=certifi.where())\n",
        "        opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=ctx))\n",
        "        urllib.request.install_opener(opener)\n",
        "        urllib.request.urlretrieve(subset_url, zip_path)\n",
        "        ok = True\n",
        "        print(\"✅ Baixado com validação TLS (urllib+certifi).\")\n",
        "    except Exception as e:\n",
        "        print(\"❌ Falhou urllib+certifi:\", e)\n",
        "\n",
        "    if not ok:\n",
        "        print(\"⬇️ Tentando com wget (sem checar certificado)...\")\n",
        "        rc = os.system(f'wget -O \"{zip_path}\" --no-check-certificate \"{subset_url}\"')\n",
        "        ok = (rc == 0)\n",
        "        print(\"✅ Baixado com wget.\" if ok else \"❌ wget falhou.\")\n",
        "\n",
        "    if not ok:\n",
        "        print(\"⬇️ Tentando com curl (sem checar certificado)...\")\n",
        "        rc = os.system(f'curl -L -k \"{subset_url}\" -o \"{zip_path}\"')\n",
        "        ok = (rc == 0)\n",
        "        print(\"✅ Baixado com curl.\" if ok else \"❌ curl falhou.\")\n",
        "\n",
        "else:\n",
        "    ok = True\n",
        "    print(\"📦 ZIP já existe. Pulando download.\")\n",
        "\n",
        "if not ok:\n",
        "    print(\"\\n⚠️ Não foi possível baixar automaticamente.\")\n",
        "    print(\"Opções alternativas:\")\n",
        "    print(\"  (a) Faça upload manual de TWICEsubset.zip na raiz (/content) e rode esta célula de novo; ou\")\n",
        "    print(\"  (b) Monte o Google Drive e copie o arquivo para /content:\")\n",
        "    print('      from google.colab import drive; drive.mount(\"/content/drive\")')\n",
        "    print('      !cp \"/content/drive/MyDrive/TWICEsubset.zip\" \"/content/TWICEsubset.zip\"')\n",
        "    raise SystemExit(1)\n",
        "\n",
        "# Verificação rápida e extração\n",
        "size_mb = os.path.getsize(zip_path) / (1024*1024)\n",
        "print(f\"📏 Tamanho do ZIP: {size_mb:.1f} MB\")\n",
        "print(\"🔐 SHA256:\", sha256sum(zip_path)[:16], \"…\")  # fingerprint curto\n",
        "\n",
        "print(\"📦 Extraindo…\")\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    bad = zf.testzip()\n",
        "    if bad:\n",
        "        raise RuntimeError(f\"Arquivo corrompido (primeiro erro em: {bad})\")\n",
        "    zf.extractall(out_dir)\n",
        "\n",
        "print(\"✅ Pronto. Dados em:\", out_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 5 — configurar caminhos e importar utilitários\n",
        "\n",
        "o que faz: define DATA_ROOT=/content/TWICE e adiciona /content/scripts no sys.path. depois importa:\n",
        "\n",
        "rotation_matrix e cuboid_corners (para compor as arestas do cubo 3D),\n",
        "\n",
        "Camera (modelo de câmera usado na projeção).\n",
        "\n",
        "relação com o TWICE: prepara o ambiente para projetar as anotações 3D (OpenLABEL) sobre os frames de câmera do TWICE e para acessar outros utilitários do repo."
      ],
      "metadata": {
        "id": "r3mJ4XiShAwO"
      },
      "id": "r3mJ4XiShAwO"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7d37b089",
      "metadata": {
        "id": "7d37b089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b54b80-25ac-4439-e776-b547777b93cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_ROOT = /content/TWICE\n",
            "SCRIPTS_ROOT = /content/scripts\n",
            "✅ utilitários importados\n"
          ]
        }
      ],
      "source": [
        "# CEL 5 — paths e imports\n",
        "import sys, pathlib\n",
        "DATA_ROOT = pathlib.Path(\"/content/TWICE\")\n",
        "SCRIPTS_ROOT = pathlib.Path(\"/content/scripts\")\n",
        "sys.path.insert(0, str(SCRIPTS_ROOT))\n",
        "\n",
        "print(\"DATA_ROOT =\", DATA_ROOT)\n",
        "print(\"SCRIPTS_ROOT =\", SCRIPTS_ROOT)\n",
        "\n",
        "from cuboid_project import rotation_matrix, cuboid_corners\n",
        "from vehicle_to_image import Camera\n",
        "print(\"✅ utilitários importados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 6 — ler frames de câmera de um arquivo .osi e gerar um vídeo preview\n",
        "\n",
        "o que faz (técnico):\n",
        "\n",
        "encontra uma pasta .../Camera/ dentro do subset.\n",
        "\n",
        "abre o .osi da câmera em modo binário e faz o loop:\n",
        "\n",
        "lê 4 bytes com struct.unpack(\"I\", ...) → isso é o tamanho da próxima mensagem protobuf,\n",
        "\n",
        "lê o próximo blob de bytes (msg_len),\n",
        "\n",
        "instancia SensorData() (classe OSI) e chama ParseFromString(msg),\n",
        "\n",
        "obtém sd.image_data[0].image.raw_data (bytes JPEG do frame),\n",
        "\n",
        "decodifica para numpy com cv2.imdecode(...).\n",
        "\n",
        "guarda ~150 frames e salva um vídeo .mp4 (15 fps) com cv2.VideoWriter.\n",
        "\n",
        "-- + robusto:\n",
        "\n",
        "Lê cada mensagem protobuf do .osi (câmera),\n",
        "\n",
        "Percorre recursivamente os campos e submensagens,\n",
        "\n",
        "Detecta bytes com “assinatura” de JPEG (FF D8 … FF D9) ou PNG (89 50 4E 47),\n",
        "\n",
        "Decodifica com OpenCV e gera o preview MP4.\n",
        "\n",
        "-- Solução do erro:\n",
        "Esse arquivo é camera_sv_… (o “sv” costuma indicar SensorView, não SensorData).\n",
        "\n",
        "Em TWICE, câmera/LiDAR foram “empacotados” como mensagens individuais dentro do .osi (loop [len][protobuf]), e algumas builds do OSI mudam os nomes/caminhos dos campos.\n",
        "\n",
        "Para resolver:\n",
        "\n",
        "Tentar SensorData, depois SensorView (duas classes diferentes do OSI).\n",
        "\n",
        "Se ainda não achar, varrer o blob inteiro procurando JPEG/PNG diretamente (SOI/EOI), e decodificar.\n",
        "\n",
        "relação com o TWICE: você reconstrói as imagens da câmera a partir do arquivo OSI do TWICE (cada mensagem é um frame), e cria um preview para inspecionar rapidamente se o caso/condição climática está ok."
      ],
      "metadata": {
        "id": "SePp97THhSxh"
      },
      "id": "SePp97THhSxh"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6459d5c7",
      "metadata": {
        "id": "6459d5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b7f276-6892-4d6e-9f4b-a2561d18de9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎥 Câmera .osi: /content/TWICE/TWICEsubset/Scenarios/dynamic_ego/CCRb/real/daytime/cluster/test_run_1/Camera/camera_sv_350_300.osi\n",
            "✅ Preview salvo em: /content/camera_preview.mp4  (151 frames)\n"
          ]
        }
      ],
      "source": [
        "# CEL 6 — iterar frames do .osi (SensorData → SensorView → varredura do blob) e salvar um MP4\n",
        "import cv2, struct\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Tentar importar classes OSI\n",
        "SensorData = None\n",
        "SensorView = None\n",
        "try:\n",
        "    from cuboid_project import SensorData as TW_SensorData\n",
        "    SensorData = TW_SensorData\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    # alguns pacotes expõem diretamente\n",
        "    import osi3\n",
        "    if SensorData is None and hasattr(osi3, \"SensorData\"):\n",
        "        SensorData = osi3.SensorData\n",
        "    # SensorView pode vir como atributo ou via módulo pb2\n",
        "    if hasattr(osi3, \"SensorView\"):\n",
        "        SensorView = osi3.SensorView\n",
        "    else:\n",
        "        try:\n",
        "            from osi3 import sensorview_pb2\n",
        "            SensorView = sensorview_pb2.SensorView\n",
        "        except:\n",
        "            pass\n",
        "except:\n",
        "    pass\n",
        "\n",
        "JPEG_SOI = b\"\\xFF\\xD8\"\n",
        "JPEG_EOI = b\"\\xFF\\xD9\"\n",
        "PNG_SIG  = b\"\\x89PNG\\r\\n\\x1a\\n\"\n",
        "\n",
        "def looks_like_img(buf: bytes) -> bool:\n",
        "    return (\n",
        "        (len(buf) > 8 and buf.startswith(JPEG_SOI) and buf.endswith(JPEG_EOI)) or\n",
        "        (len(buf) > 8 and buf.startswith(PNG_SIG)) or\n",
        "        (len(buf) > 8 and buf.startswith(JPEG_SOI) and b\"\\xFF\\xD9\" in buf[-4:])\n",
        "    )\n",
        "\n",
        "def scan_blob_for_image(blob: bytes):\n",
        "    # Procura um JPEG (SOI...EOI)\n",
        "    start = blob.find(JPEG_SOI)\n",
        "    if start != -1:\n",
        "        end = blob.find(JPEG_EOI, start+2)\n",
        "        if end != -1:\n",
        "            return blob[start:end+2]\n",
        "    # Procura um PNG\n",
        "    start = blob.find(PNG_SIG)\n",
        "    if start != -1:\n",
        "        # PNG é chunked; tentamos até o fim do blob\n",
        "        return blob[start:]\n",
        "    return None\n",
        "\n",
        "def extract_img_from_any_message(msg):\n",
        "    # Percorre recursivamente a mensagem protobuf atrás de campos bytes que pareçam imagem\n",
        "    try:\n",
        "        fields = msg.ListFields()\n",
        "    except Exception:\n",
        "        return None\n",
        "    from google.protobuf.descriptor import FieldDescriptor\n",
        "    for desc, val in fields:\n",
        "        try:\n",
        "            if desc.type == FieldDescriptor.TYPE_BYTES and looks_like_img(val):\n",
        "                return val\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Submensagem\n",
        "        if hasattr(val, \"ListFields\"):\n",
        "            out = extract_img_from_any_message(val)\n",
        "            if out is not None:\n",
        "                return out\n",
        "        # Lista de submensagens\n",
        "        if isinstance(val, (list, tuple)):\n",
        "            for elem in val:\n",
        "                if hasattr(elem, \"ListFields\"):\n",
        "                    out = extract_img_from_any_message(elem)\n",
        "                    if out is not None:\n",
        "                        return out\n",
        "    return None\n",
        "\n",
        "def iter_camera_frames(osi_path: Path):\n",
        "    with open(osi_path, \"rb\") as f:\n",
        "        while True:\n",
        "            len_bytes = f.read(4)\n",
        "            if not len_bytes:\n",
        "                break\n",
        "            (msg_len,) = struct.unpack(\"I\", len_bytes)\n",
        "            blob = f.read(msg_len)\n",
        "\n",
        "            # a) tentar SensorData\n",
        "            if SensorData is not None:\n",
        "                try:\n",
        "                    sd = SensorData()\n",
        "                    sd.ParseFromString(blob)\n",
        "                    img_bytes = extract_img_from_any_message(sd)\n",
        "                    if img_bytes:\n",
        "                        img_np = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "                        frame = cv2.imdecode(img_np, cv2.IMREAD_COLOR)\n",
        "                        if frame is not None:\n",
        "                            yield frame\n",
        "                            continue\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            # b) tentar SensorView\n",
        "            if SensorView is not None:\n",
        "                try:\n",
        "                    sv = SensorView()\n",
        "                    sv.ParseFromString(blob)\n",
        "                    img_bytes = extract_img_from_any_message(sv)\n",
        "                    if img_bytes:\n",
        "                        img_np = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "                        frame = cv2.imdecode(img_np, cv2.IMREAD_COLOR)\n",
        "                        if frame is not None:\n",
        "                            yield frame\n",
        "                            continue\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            # c) fallback bruto: vasculhar o blob por JPEG/PNG\n",
        "            img_bytes = scan_blob_for_image(blob)\n",
        "            if img_bytes:\n",
        "                img_np = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "                frame = cv2.imdecode(img_np, cv2.IMREAD_COLOR)\n",
        "                if frame is not None:\n",
        "                    yield frame\n",
        "                    continue\n",
        "            # Se chegou aqui, essa mensagem não tinha imagem (ou formato não suportado). Continua.\n",
        "\n",
        "# Localizar o arquivo de Câmera (você já tinha cam_file)\n",
        "scenario_dir = next(DATA_ROOT.rglob(\"Camera\"), None)\n",
        "assert scenario_dir is not None, \"Não encontrei /Camera no subset. Verifique a extração do ZIP.\"\n",
        "cam_file = next(Path(scenario_dir).glob(\"*.osi\"))\n",
        "print(\"🎥 Câmera .osi:\", cam_file)\n",
        "\n",
        "frames = []\n",
        "for i, fr in enumerate(iter_camera_frames(cam_file)):\n",
        "    frames.append(fr)\n",
        "    if i >= 150:\n",
        "        break\n",
        "\n",
        "if not frames:\n",
        "    raise RuntimeError(\"Ainda não consegui extrair frames. Rode a célula de DIAGNÓSTICO para inspecionar os campos.\")\n",
        "\n",
        "h, w = frames[0].shape[:2]\n",
        "out_path = \"/content/camera_preview.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "vw = cv2.VideoWriter(out_path, fourcc, 15, (w, h))\n",
        "for fr in frames:\n",
        "    vw.write(fr)\n",
        "vw.release()\n",
        "print(f\"✅ Preview salvo em: {out_path}  ({len(frames)} frames)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 7 — carregar OpenLABEL e preparar a projeção 3D→2D\n",
        "\n",
        "o que faz:\n",
        "\n",
        "abre o *open*label*camera*.json do cenário,\n",
        "\n",
        "extrai a matriz intrínseca K a partir de camera_matrix_3x4 → remodela para (3×4) e usa as 3×3 primeiras colunas,\n",
        "\n",
        "(na célula de exemplo) só escreve um texto no frame; para desenhar as arestas do cubo você usa:\n",
        "\n",
        "os “cuboids” (posição, dimensões, orientação) em openlabel[\"objects\"][...][\"object_data\"][\"cuboid\"][\"val\"],\n",
        "\n",
        "funções do cuboid_project.py para obter os 8 cantos 3D (cuboid_corners), aplicar rotação/translação (extrínsecas) e projetar com K (intrínsecas),\n",
        "\n",
        "desenhar arestas com cv2.line.\n",
        "\n",
        "-- Solução: isso acontece porque alguns arquivos OpenLABEL do TWICE não usam a chave intrinsics_pinhole (ela aparece no exemplo do artigo, mas no subset real pode estar com outro nome/caminho).\n",
        "\n",
        "CEL 7 substituta, mais robusta, que:\n",
        "\n",
        "carrega o JSON do OpenLABEL,\n",
        "\n",
        "nesse JSON do OpenLABEL as intrínsecas não estão gravadas (só há timestamp e Streams). Sem dramas: dá pra seguir de duas formas:\n",
        "\n",
        "Procurar mais fundo dentro de Streams.Camera1 (às vezes a matriz aparece com outro nome/caminho).\n",
        "\n",
        "Fallback físico: estimar K a partir do FOV + tamanho da imagem. O artigo informa ~60° (horizontal) e ~36.6° (vertical) para a Sekonix 1920×1232 — isso é suficiente para projetar cubos com boa aproximação.\n",
        "\n",
        "procura recursivamente por matrizes intrínsecas em formatos comuns:\n",
        "\n",
        "camera_matrix_3x4 (tira a parte 3×3),\n",
        "\n",
        "K (3×3),\n",
        "\n",
        "campos soltos fx, fy, cx, cy,\n",
        "\n",
        "outras variações como camera_matrix, intrinsics, etc.\n",
        "\n",
        "imprime qual caminho foi encontrado,\n",
        "\n",
        "desenha um texto no primeiro frame (só para confirmar; depois a gente pluga no desenho 3D→2D).\n",
        "\n",
        "relação com o TWICE: o TWICE rotula em OpenLABEL (padrão da ASAM): você aproveita essas anotações para desenhar ground-truth sobre os frames, checar calibração e gerar vídeos anotados."
      ],
      "metadata": {
        "id": "bd-uyT-wiOS_"
      },
      "id": "bd-uyT-wiOS_"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "588ebaad",
      "metadata": {
        "id": "588ebaad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1cf40e-f894-4f1a-b2de-374e646e112a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧾 OpenLABEL: /content/TWICE/TWICEsubset/Scenarios/dynamic_ego/CCRb/real/daytime/cluster/test_run_1/Camera/open_label_camera.json\n",
            "✅ K usada: fallback FOV (60.0° × 36.6°) + frame 1920×1208\n",
            "[[1.66276878e+03 0.00000000e+00 9.60000000e+02]\n",
            " [0.00000000e+00 1.82632728e+03 6.04000000e+02]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
            "🖼️ Exemplo salvo em /content/cuboids_demo.jpg\n"
          ]
        }
      ],
      "source": [
        "# CEL 7 (robusta v2) — localizar K em Streams.Camera1 OU estimar por FOV (fallback)\n",
        "import json, cv2, numpy as np, math\n",
        "from pathlib import Path\n",
        "\n",
        "openlabel_file = next(Path(scenario_dir).glob(\"*open*label*camera*.json\"), None)\n",
        "print(\"🧾 OpenLABEL:\", openlabel_file)\n",
        "\n",
        "with open(openlabel_file, \"r\") as f:\n",
        "    ol = json.load(f)\n",
        "\n",
        "# 1) pega o nó Camera1 (se existir)\n",
        "cam1 = {}\n",
        "try:\n",
        "    cam1 = ol[\"openlabel\"][\"frames\"][0][\"frame_properties\"][\"Streams\"][\"Camera1\"]\n",
        "except Exception:\n",
        "    cam1 = {}\n",
        "\n",
        "def _flatten(d, prefix=\"\"):\n",
        "    if isinstance(d, dict):\n",
        "        for k,v in d.items():\n",
        "            p = f\"{prefix}.{k}\" if prefix else k\n",
        "            yield from _flatten(v, p)\n",
        "    elif isinstance(d, list):\n",
        "        for i,v in enumerate(d):\n",
        "            p = f\"{prefix}[{i}]\"\n",
        "            yield from _flatten(v, p)\n",
        "    else:\n",
        "        yield prefix, d\n",
        "\n",
        "def _find_K_in_dict(node):\n",
        "    # procura 3x4 -> 3x3\n",
        "    for path, val in _flatten(node):\n",
        "        if isinstance(val, list) and len(val)==12 and all(isinstance(x,(int,float)) for x in val):\n",
        "            if any(tok in path.lower() for tok in [\"camera_matrix_3x4\",\"matrix_3x4\",\"3x4\",\"pinhole\"]):\n",
        "                K = np.array(val, dtype=float).reshape(3,4)[:,:3]\n",
        "                return K, f\"{path} (3x4→K)\"\n",
        "    # procura 3x3 direta\n",
        "    for path, val in _flatten(node):\n",
        "        if isinstance(val, list) and len(val)==9 and all(isinstance(x,(int,float)) for x in val):\n",
        "            if any(tok in path.lower() for tok in [\"camera_matrix\",\"intrinsics\",\"k\",\"matrix_3x3\",\"3x3\"]):\n",
        "                K = np.array(val, dtype=float).reshape(3,3)\n",
        "                return K, f\"{path} (3x3)\"\n",
        "    # fx,fy,cx,cy no mesmo nível\n",
        "    def _fxfy_from_dict(d):\n",
        "        if not isinstance(d, dict): return None\n",
        "        keys = {k.lower():k for k in d.keys()}\n",
        "        need = [\"fx\",\"fy\",\"cx\",\"cy\"]\n",
        "        if all(n in keys for n in need):\n",
        "            try:\n",
        "                fx = float(d[keys[\"fx\"]]); fy = float(d[keys[\"fy\"]])\n",
        "                cx = float(d[keys[\"cx\"]]); cy = float(d[keys[\"cy\"]])\n",
        "                K  = np.array([[fx,0,cx],[0,fy,cy],[0,0,1]],dtype=float)\n",
        "                return K\n",
        "            except: return None\n",
        "        return None\n",
        "    def _walk(d, path=\"\"):\n",
        "        if isinstance(d, dict):\n",
        "            K = _fxfy_from_dict(d)\n",
        "            if K is not None:\n",
        "                return K, f\"{path or 'fx/fy/cx/cy'}\"\n",
        "            for k,v in d.items():\n",
        "                out = _walk(v, f\"{path}.{k}\" if path else k)\n",
        "                if out: return out\n",
        "        elif isinstance(d, list):\n",
        "            for i,v in enumerate(d):\n",
        "                out = _walk(v, f\"{path}[{i}]\")\n",
        "                if out: return out\n",
        "        return None\n",
        "    out = _walk(node)\n",
        "    if out: return out\n",
        "    return None, None\n",
        "\n",
        "K, where = _find_K_in_dict(cam1)\n",
        "\n",
        "if K is None:\n",
        "    # 2) Fallback: estimar K por FOV (do paper) + tamanho do frame\n",
        "    # pegue dimensões do frame já decodificado na CEL 6\n",
        "    H, W = frames[0].shape[:2]\n",
        "    # Tenta ler FOV do JSON (se existir); caso contrário, usa nominal 60° × 36.6°\n",
        "    hfov_deg = 60.0\n",
        "    vfov_deg = 36.6\n",
        "    # procure possíveis campos de FOV:\n",
        "    for path,val in _flatten(cam1):\n",
        "        if isinstance(val,(int,float)) and \"fov\" in path.lower():\n",
        "            # heurística: se encontrar dois floats, assume primeiro horizontal e segundo vertical\n",
        "            if \"horizontal\" in path.lower(): hfov_deg = float(val)\n",
        "            if \"vertical\"   in path.lower(): vfov_deg = float(val)\n",
        "    # converte para radianos\n",
        "    hfov = math.radians(hfov_deg)\n",
        "    vfov = math.radians(vfov_deg)\n",
        "    # modelo pinhole: fx = (W/2)/tan(hfov/2); fy = (H/2)/tan(vfov/2)\n",
        "    fx = (W/2.0) / math.tan(hfov/2.0)\n",
        "    fy = (H/2.0) / math.tan(vfov/2.0)\n",
        "    cx = W/2.0\n",
        "    cy = H/2.0\n",
        "    K = np.array([[fx,0,cx],[0,fy,cy],[0,0,1]], dtype=float)\n",
        "    where = f\"fallback FOV ({hfov_deg:.1f}° × {vfov_deg:.1f}°) + frame {W}×{H}\"\n",
        "\n",
        "print(\"✅ K usada:\", where)\n",
        "print(K)\n",
        "\n",
        "# Marcação simples no primeiro frame só para confirmar\n",
        "frame0 = frames[0].copy()\n",
        "cv2.putText(frame0, f\"K: {where}\", (30,40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
        "cv2.imwrite(\"/content/cuboids_demo.jpg\", frame0)\n",
        "print(\"🖼️ Exemplo salvo em /content/cuboids_demo.jpg\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 8 — visualizar radar (cluster / object list) e trajetórias\n",
        "\n",
        "o que faz: importa radar_osi_plot.py, localiza um .osi de radar (se existir no subset daquele cenário) e orienta usar as funções do script para plotar:\n",
        "\n",
        "cluster: nuvem de pontos (cada reflexão do radar),\n",
        "\n",
        "object list: lista de objetos agregados pelo ECU (com velocidade/posição/RCS),\n",
        "\n",
        "posição do ego e do alvo (IMU/GNSS) ao longo do tempo.\n",
        "\n",
        "relação com o TWICE: permite ver como o radar “enxerga” o mesmo cenário da câmera (real e HIL), inclusive comparando modos de operação."
      ],
      "metadata": {
        "id": "qqHXrOwhi63l"
      },
      "id": "qqHXrOwhi63l"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1531b258",
      "metadata": {
        "id": "1531b258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611cfc7a-ef3b-44af-e538-4999c057926b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📡 Radar .osi: /content/TWICE/TWICEsubset/Scenarios/dynamic_ego/CCRb/real/daytime/cluster/test_run_1/Radar/radar_sd_350_300.osi\n"
          ]
        }
      ],
      "source": [
        "# CEL 8 — visualizar radar (cluster/object list) — precisa de arquivo em /Radar\n",
        "from importlib import reload\n",
        "import os\n",
        "try:\n",
        "    import radar_osi_plot as rplt\n",
        "    reload(rplt)\n",
        "    radar_dir = (Path(scenario_dir).parent / \"Radar\")\n",
        "    if radar_dir.exists():\n",
        "        radar_osi = next(radar_dir.glob(\"*.osi\"))\n",
        "        print(\"📡 Radar .osi:\", radar_osi)\n",
        "        # Abra o arquivo radar_osi_plot.py para ver como salvar vídeo (há exemplo lá).\n",
        "    else:\n",
        "        print(\"⚠️ Este caso do subset pode não ter Radar. Teste outro cenário ou use o dataset completo.\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Import do script de radar falhou:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 9 — (opcional) rodar YOLOv7 como demo\n",
        "\n",
        "o que faz: exporta ~30 frames do preview para /content/frames/*.jpg, clona o repositório do YOLOv7 e roda detect.py com yolov7.pt. salva imagens e labels preditos em /content/y7out/det.\n",
        "\n",
        "instala PyTorch já compilado (CUDA 12.1, CPU fallback), clona o YOLOv7 e roda o detect.py.\n",
        "\n",
        "relação com o TWICE: demonstra um baseline de detecção sobre os frames do TWICE. a partir daí, você pode comparar com GT do OpenLABEL (convertendo cuboids 3D → caixas 2D) para calcular IoU/AP, como o artigo exemplifica."
      ],
      "metadata": {
        "id": "jOW9TCajjTbp"
      },
      "id": "jOW9TCajjTbp"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1b928cb8",
      "metadata": {
        "id": "1b928cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e009c4d-ba54-4e68-fb71-b95f5f670922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "⬇️ Instalando PyTorch + TorchVision (CUDA 12.1 wheels)...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m707.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m/content/yolov7\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy<1.24.0,>=1.18.5 (from versions: 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy<1.24.0,>=1.18.5\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "/content\n",
            "/content/yolov7\n",
            "Namespace(weights=['yolov7.pt'], source='/content/frames', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=True, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='/content/y7out', name='det', exist_ok=True, no_trace=False)\n",
            "YOLOR 🚀 v0.1-128-ga207844 torch 2.4.1+cu121 CPU\n",
            "\n",
            "Downloading https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt to yolov7.pt...\n",
            "100% 72.1M/72.1M [00:00<00:00, 131MB/s]\n",
            "\n",
            "/content/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(w, map_location=map_location)  # load\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "Model Summary: 306 layers, 36905341 parameters, 6652669 gradients\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "1 person, 1 car, Done. (1075.0ms) Inference, (1.0ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0000.jpg\n",
            "1 car, Done. (1001.8ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0001.jpg\n",
            "1 car, Done. (1009.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0002.jpg\n",
            "1 car, Done. (1005.0ms) Inference, (1.1ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0003.jpg\n",
            "1 person, 1 car, Done. (1250.5ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0004.jpg\n",
            "1 car, Done. (1002.1ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0005.jpg\n",
            "1 car, Done. (1009.1ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0006.jpg\n",
            "1 car, Done. (999.9ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0007.jpg\n",
            "1 car, Done. (991.9ms) Inference, (0.6ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0008.jpg\n",
            "1 person, 1 car, 1 truck, Done. (1008.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0009.jpg\n",
            "1 car, Done. (1012.4ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0010.jpg\n",
            "1 car, Done. (1003.7ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0011.jpg\n",
            "1 car, Done. (1004.5ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0012.jpg\n",
            "1 car, Done. (996.2ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0013.jpg\n",
            "1 car, Done. (1091.2ms) Inference, (0.9ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0014.jpg\n",
            "1 car, Done. (1165.8ms) Inference, (0.6ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0015.jpg\n",
            "1 car, 1 truck, Done. (990.7ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0016.jpg\n",
            "1 car, Done. (992.7ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0017.jpg\n",
            "1 car, Done. (999.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0018.jpg\n",
            "1 car, Done. (997.5ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0019.jpg\n",
            "1 car, Done. (1036.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0020.jpg\n",
            "1 car, Done. (996.9ms) Inference, (0.6ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0021.jpg\n",
            "1 car, Done. (998.9ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0022.jpg\n",
            "1 car, Done. (992.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0023.jpg\n",
            "1 car, Done. (993.5ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0024.jpg\n",
            "1 car, Done. (1159.3ms) Inference, (0.9ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0025.jpg\n",
            "1 car, Done. (1116.6ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0026.jpg\n",
            "1 car, Done. (999.4ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0027.jpg\n",
            "1 car, Done. (1016.4ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0028.jpg\n",
            "1 car, Done. (1013.6ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0029.jpg\n",
            "Done. (31.547s)\n",
            "✅ Resultados em:\n",
            "  - imagens: /content/y7out/det\n",
            "  - labels:  /content/y7out/det/labels\n"
          ]
        }
      ],
      "source": [
        "# === YOLOv7 - instalação robusta e inferência nos frames ===\n",
        "import os, cv2, pathlib, sys, subprocess, textwrap\n",
        "\n",
        "# 0) Garantir que estamos na raiz do Colab\n",
        "%cd /content\n",
        "\n",
        "# 1) Clonar repositório yolov7 (se ainda não existe)\n",
        "if not pathlib.Path(\"/content/yolov7\").exists():\n",
        "    !git clone -q https://github.com/WongKinYiu/yolov7.git\n",
        "else:\n",
        "    print(\"🔁 yolov7 já clonado.\")\n",
        "\n",
        "# 2) Instalar PyTorch pré-compilado (tenta CUDA 12.1; se falhar, cai para CPU)\n",
        "def install_torch():\n",
        "    try:\n",
        "        print(\"⬇️ Instalando PyTorch + TorchVision (CUDA 12.1 wheels)...\")\n",
        "        !pip -q install --upgrade \"torch>=2.2,<3\" \"torchvision>=0.17,<0.20\" --index-url https://download.pytorch.org/whl/cu121\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Falhou CUDA 12.1, tentando CPU wheels…\")\n",
        "        !pip -q install --upgrade \"torch>=2.2,<3\" \"torchvision>=0.17,<0.20\" --index-url https://download.pytorch.org/whl/cpu\n",
        "install_torch()\n",
        "\n",
        "# 3) Instalar dependências do YOLOv7 (ignorando torch, que já instalamos acima)\n",
        "%cd /content/yolov7\n",
        "# remove linhas de torch do requirements (se houver) para evitar conflitos\n",
        "!grep -viE 'torch|torchvision' requirements.txt > /content/yolov7/req_no_torch.txt\n",
        "# instalar com wheels quando possível (evita compilar)\n",
        "!pip -q install -r req_no_torch.txt --only-binary=:all: || pip -q install -r req_no_torch.txt\n",
        "\n",
        "# 4) Preparar frames (caso ainda não tenha sido feito)\n",
        "%cd /content\n",
        "import os, cv2\n",
        "os.makedirs(\"/content/frames\", exist_ok=True)\n",
        "if len(os.listdir(\"/content/frames\")) == 0:\n",
        "    # usa os frames coletados na CEL 6 (lista 'frames') para exportar ~30 imagens\n",
        "    for i, fr in enumerate(frames[:30]):\n",
        "        cv2.imwrite(f\"/content/frames/{i:04d}.jpg\", fr)\n",
        "    print(\"🖼️ Exportei\", min(30, len(frames)), \"frames para /content/frames\")\n",
        "\n",
        "# 5) Rodar detecção\n",
        "%cd /content/yolov7\n",
        "!python detect.py --weights yolov7.pt --conf 0.25 --source /content/frames --save-txt --project /content/y7out --name det --exist-ok\n",
        "\n",
        "print(\"✅ Resultados em:\")\n",
        "print(\"  - imagens: /content/y7out/det\")\n",
        "print(\"  - labels:  /content/y7out/det/labels\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
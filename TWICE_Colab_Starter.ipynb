{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomanGustavo/Mestrado----Projeto-Carro/blob/main/TWICE_Colab_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905f97d3",
      "metadata": {
        "id": "905f97d3"
      },
      "source": [
        "# TWICE Dataset â€” Colab Starter\n",
        "Este notebook prepara o ambiente, baixa o **subset** do TWICE, lÃª `.osi`, projeta cuboids (via utilitÃ¡rios do repo) e mostra como visualizar radar e rodar um detector (YOLOv7) para DEMO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 1 â€” instalar pacotes\n",
        "\n",
        "o que faz: pip install de dependÃªncias usadas ao longo do notebook:\n",
        "\n",
        "opencv-python (ler/decodificar imagens JPEG dentro dos .osi, fazer vÃ­deo),\n",
        "\n",
        "tqdm (barrinhas de progresso),\n",
        "\n",
        "open3d (visualizar/grav ar nuvem de pontos .ply â€” Ãºtil pro LiDAR),\n",
        "\n",
        "protobuf==4.25.3 (compatÃ­vel com as mensagens OSI),\n",
        "\n",
        "matplotlib (grÃ¡ficos simples).\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: os arquivos do TWICE usam OSI via protobuf; sem o protobuf correto e o OpenCV, vocÃª nÃ£o abre os frames da cÃ¢mera nem gera os vÃ­deos."
      ],
      "metadata": {
        "id": "i7nu_XHJfPPy"
      },
      "id": "i7nu_XHJfPPy"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "acb83af7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "acb83af7",
        "outputId": "f9534b9f-0e11-4889-8d8d-1c8f08f0291b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==4.25.3\n",
            "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.3\n",
            "    Uninstalling protobuf-4.25.3:\n",
            "      Successfully uninstalled protobuf-4.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "e553b75ef0a74f9f91495f76d8c6621c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grpcio-status==1.60.0 in /usr/local/lib/python3.12/dist-packages (1.60.0)\n",
            "Requirement already satisfied: protobuf>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from grpcio-status==1.60.0) (4.25.3)\n",
            "Requirement already satisfied: grpcio>=1.60.0 in /usr/local/lib/python3.12/dist-packages (from grpcio-status==1.60.0) (1.74.0)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.5.5 in /usr/local/lib/python3.12/dist-packages (from grpcio-status==1.60.0) (1.70.0)\n"
          ]
        }
      ],
      "source": [
        "# CEL 1 â€” pacotes base\n",
        "!pip -q install --upgrade opencv-python tqdm open3d protobuf==4.25.3 matplotlib\n",
        "\n",
        "# ForÃ§ar protobuf 4 sÃ³ para os scripts OSI/TWICE\n",
        "!pip install \"protobuf==4.25.3\" --force-reinstall\n",
        "\n",
        "# Depois reinstalar o grpcio-status na versÃ£o compatÃ­vel com protobuf 4\n",
        "!pip install \"grpcio-status==1.60.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 2 â€” instalar/ativar o OSI (Open Simulation Interface)\n",
        "\n",
        "o que faz: tenta import osi3. se nÃ£o existir, clona o repositÃ³rio oficial do OSI, gera os mÃ³dulos python a partir dos .proto (com setup.py build_py) e instala com pip.\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: os .osi do TWICE sÃ£o mensagens protobuf do OSI (classes como SensorData e SensorDataSeries). sem o pacote osi3, vocÃª nÃ£o consegue decodificar o conteÃºdo dos arquivos de cÃ¢mera/radar/lidar/IMU."
      ],
      "metadata": {
        "id": "CWU6Ug0Oe-60"
      },
      "id": "CWU6Ug0Oe-60"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e0637233",
      "metadata": {
        "id": "e0637233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd37edd-eb9c-456d-b42e-41fd9cdb3064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… osi3 jÃ¡ disponÃ­vel\n"
          ]
        }
      ],
      "source": [
        "# CEL 2 â€” Open Simulation Interface (OSI): tenta importar; se nÃ£o existir, compila a partir do repo oficial\n",
        "try:\n",
        "    import osi3\n",
        "    print(\"âœ… osi3 jÃ¡ disponÃ­vel\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ osi3 nÃ£o encontrado. Clonando OSI e instalando a partir do cÃ³digo-fonte...\")\n",
        "    !git clone -q https://github.com/OpenSimulationInterface/open-simulation-interface.git\n",
        "    %cd /content/open-simulation-interface\n",
        "    !python3 setup.py -q build_py\n",
        "    !pip -q install .\n",
        "    %cd /content\n",
        "    import osi3\n",
        "    print(\"âœ… osi3 instalado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 3 â€” clonar os scripts do repositÃ³rio TWICE\n",
        "\n",
        "o que faz: git clone de TWICEdataset/scripts e entra na pasta.\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: esse repo traz utilitÃ¡rios prontos que jÃ¡ entendem a estrutura OSI/OpenLABEL do TWICE:\n",
        "\n",
        "cuboid_project.py: projeÃ§Ã£o de cuboids 3D â†’ imagem (usa intrÃ­nsecas/extrÃ­nsecas).\n",
        "\n",
        "radar_osi_plot.py: leitura/plot do radar (modos cluster e object list) + trajetÃ³ria.\n",
        "\n",
        "lidar_osi2PLY.py: converter LiDAR .osi â†’ .ply.\n",
        "\n",
        "tutorial.ipynb: exemplos prontos."
      ],
      "metadata": {
        "id": "Ek8fyQGZgYqw"
      },
      "id": "Ek8fyQGZgYqw"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fde90579",
      "metadata": {
        "id": "fde90579",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea90f49-d9fb-4306-d813-ad1b3ddb9c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/scripts\n",
            "total 136\n",
            "drwxr-xr-x 6 root root  4096 Aug 24 14:56 .\n",
            "drwxr-xr-x 1 root root  4096 Aug 24 14:15 ..\n",
            "-rw-r--r-- 1 root root 16496 Aug 24 14:15 cuboid_project.py\n",
            "-rw-r--r-- 1 root root  4891 Aug 24 14:15 file_selector.py\n",
            "drwxr-xr-x 8 root root  4096 Aug 24 14:15 .git\n",
            "-rw-r--r-- 1 root root  3317 Aug 24 14:15 lidar_osi2PLY.py\n",
            "drwxr-xr-x 2 root root  4096 Aug 24 14:15 __pycache__\n",
            "-rw-r--r-- 1 root root 27662 Aug 24 14:15 radar_osi_plot.py\n",
            "-rw-r--r-- 1 root root  1545 Aug 24 14:15 README.md\n",
            "-rw-r--r-- 1 root root   117 Aug 24 14:15 requirements.txt\n",
            "drwxr-xr-x 2 root root  4096 Aug 24 14:15 Results\n",
            "drwxr-xr-x 5 root root  4096 Aug 24 14:56 scripts\n",
            "-rw-r--r-- 1 root root 18228 Aug 24 14:15 tutorial.ipynb\n",
            "-rw-r--r-- 1 root root 15554 Aug 24 14:15 TWICE_df.csv\n",
            "-rw-r--r-- 1 root root    20 Aug 24 14:15 TWICE_path.txt\n",
            "-rw-r--r-- 1 root root  2350 Aug 24 14:15 vehicle_to_image.py\n"
          ]
        }
      ],
      "source": [
        "# CEL 3 â€” clonar scripts\n",
        "!git clone -q https://github.com/TWICEdataset/scripts.git\n",
        "%cd /content/scripts\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 4 â€” baixar e extrair o subset do dataset\n",
        "\n",
        "o que faz: baixa TWICEsubset.zip (~439 MB) do servidor e extrai em /content/TWICE.\n",
        "\n",
        "CEL 4 â€œrobutizadaâ€\n",
        "\n",
        "urllib + certifi (mantÃ©m validaÃ§Ã£o TLS)\n",
        "\n",
        "wget com --no-check-certificate (ignora verificaÃ§Ã£o se 1 falhar)\n",
        "\n",
        "curl -k (fallback final, tambÃ©m ignora verificaÃ§Ã£o)\n",
        "\n",
        "Depois ela verifica o arquivo, mostra tamanho e extrai\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: vocÃª passa a ter localmente a Ã¡rvore de cenÃ¡rios do dataset (ex.: Scenarios/dynamic ego/.../real/daytime/.../{Camera,Radar,LiDAR,IMU ...}), com arquivos .osi por sensor e arquivos OpenLABEL .json (anotaÃ§Ãµes e calibraÃ§Ãµes)."
      ],
      "metadata": {
        "id": "5oLcm_2rgsIG"
      },
      "id": "5oLcm_2rgsIG"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "878f3158",
      "metadata": {
        "id": "878f3158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3783650-bda5-4f4d-922f-70303cd9acc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸ Tentando baixar com urllib + certifi (TLS verificado)...\n",
            "âŒ Falhou urllib+certifi: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)>\n",
            "â¬‡ï¸ Tentando com wget (sem checar certificado)...\n",
            "âœ… Baixado com wget.\n",
            "ğŸ“ Tamanho do ZIP: 438.5 MB\n",
            "ğŸ” SHA256: b3469614e3e95491 â€¦\n",
            "ğŸ“¦ Extraindoâ€¦\n",
            "âœ… Pronto. Dados em: /content/TWICE\n"
          ]
        }
      ],
      "source": [
        "# CEL 4 (robusta) â€” baixar o subset e extrair, contornando SSL do servidor\n",
        "import os, zipfile, pathlib, hashlib, sys\n",
        "\n",
        "subset_url = \"https://twice.eletrica.ufpr.br/TWICEsubset.zip\"\n",
        "zip_path = \"/content/TWICEsubset.zip\"\n",
        "out_dir = pathlib.Path(\"/content/TWICE\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def sha256sum(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "ok = False\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"â¬‡ï¸ Tentando baixar com urllib + certifi (TLS verificado)...\")\n",
        "    try:\n",
        "        import ssl, certifi, urllib.request\n",
        "        ctx = ssl.create_default_context(cafile=certifi.where())\n",
        "        opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=ctx))\n",
        "        urllib.request.install_opener(opener)\n",
        "        urllib.request.urlretrieve(subset_url, zip_path)\n",
        "        ok = True\n",
        "        print(\"âœ… Baixado com validaÃ§Ã£o TLS (urllib+certifi).\")\n",
        "    except Exception as e:\n",
        "        print(\"âŒ Falhou urllib+certifi:\", e)\n",
        "\n",
        "    if not ok:\n",
        "        print(\"â¬‡ï¸ Tentando com wget (sem checar certificado)...\")\n",
        "        rc = os.system(f'wget -O \"{zip_path}\" --no-check-certificate \"{subset_url}\"')\n",
        "        ok = (rc == 0)\n",
        "        print(\"âœ… Baixado com wget.\" if ok else \"âŒ wget falhou.\")\n",
        "\n",
        "    if not ok:\n",
        "        print(\"â¬‡ï¸ Tentando com curl (sem checar certificado)...\")\n",
        "        rc = os.system(f'curl -L -k \"{subset_url}\" -o \"{zip_path}\"')\n",
        "        ok = (rc == 0)\n",
        "        print(\"âœ… Baixado com curl.\" if ok else \"âŒ curl falhou.\")\n",
        "\n",
        "else:\n",
        "    ok = True\n",
        "    print(\"ğŸ“¦ ZIP jÃ¡ existe. Pulando download.\")\n",
        "\n",
        "if not ok:\n",
        "    print(\"\\nâš ï¸ NÃ£o foi possÃ­vel baixar automaticamente.\")\n",
        "    print(\"OpÃ§Ãµes alternativas:\")\n",
        "    print(\"  (a) FaÃ§a upload manual de TWICEsubset.zip na raiz (/content) e rode esta cÃ©lula de novo; ou\")\n",
        "    print(\"  (b) Monte o Google Drive e copie o arquivo para /content:\")\n",
        "    print('      from google.colab import drive; drive.mount(\"/content/drive\")')\n",
        "    print('      !cp \"/content/drive/MyDrive/TWICEsubset.zip\" \"/content/TWICEsubset.zip\"')\n",
        "    raise SystemExit(1)\n",
        "\n",
        "# VerificaÃ§Ã£o rÃ¡pida e extraÃ§Ã£o\n",
        "size_mb = os.path.getsize(zip_path) / (1024*1024)\n",
        "print(f\"ğŸ“ Tamanho do ZIP: {size_mb:.1f} MB\")\n",
        "print(\"ğŸ” SHA256:\", sha256sum(zip_path)[:16], \"â€¦\")  # fingerprint curto\n",
        "\n",
        "print(\"ğŸ“¦ Extraindoâ€¦\")\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    bad = zf.testzip()\n",
        "    if bad:\n",
        "        raise RuntimeError(f\"Arquivo corrompido (primeiro erro em: {bad})\")\n",
        "    zf.extractall(out_dir)\n",
        "\n",
        "print(\"âœ… Pronto. Dados em:\", out_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 5 â€” configurar caminhos e importar utilitÃ¡rios\n",
        "\n",
        "o que faz: define DATA_ROOT=/content/TWICE e adiciona /content/scripts no sys.path. depois importa:\n",
        "\n",
        "rotation_matrix e cuboid_corners (para compor as arestas do cubo 3D),\n",
        "\n",
        "Camera (modelo de cÃ¢mera usado na projeÃ§Ã£o).\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: prepara o ambiente para projetar as anotaÃ§Ãµes 3D (OpenLABEL) sobre os frames de cÃ¢mera do TWICE e para acessar outros utilitÃ¡rios do repo."
      ],
      "metadata": {
        "id": "r3mJ4XiShAwO"
      },
      "id": "r3mJ4XiShAwO"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7d37b089",
      "metadata": {
        "id": "7d37b089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b54b80-25ac-4439-e776-b547777b93cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_ROOT = /content/TWICE\n",
            "SCRIPTS_ROOT = /content/scripts\n",
            "âœ… utilitÃ¡rios importados\n"
          ]
        }
      ],
      "source": [
        "# CEL 5 â€” paths e imports\n",
        "import sys, pathlib\n",
        "DATA_ROOT = pathlib.Path(\"/content/TWICE\")\n",
        "SCRIPTS_ROOT = pathlib.Path(\"/content/scripts\")\n",
        "sys.path.insert(0, str(SCRIPTS_ROOT))\n",
        "\n",
        "print(\"DATA_ROOT =\", DATA_ROOT)\n",
        "print(\"SCRIPTS_ROOT =\", SCRIPTS_ROOT)\n",
        "\n",
        "from cuboid_project import rotation_matrix, cuboid_corners\n",
        "from vehicle_to_image import Camera\n",
        "print(\"âœ… utilitÃ¡rios importados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 6 â€” ler frames de cÃ¢mera de um arquivo .osi e gerar um vÃ­deo preview\n",
        "\n",
        "o que faz (tÃ©cnico):\n",
        "\n",
        "encontra uma pasta .../Camera/ dentro do subset.\n",
        "\n",
        "abre o .osi da cÃ¢mera em modo binÃ¡rio e faz o loop:\n",
        "\n",
        "lÃª 4 bytes com struct.unpack(\"I\", ...) â†’ isso Ã© o tamanho da prÃ³xima mensagem protobuf,\n",
        "\n",
        "lÃª o prÃ³ximo blob de bytes (msg_len),\n",
        "\n",
        "instancia SensorData() (classe OSI) e chama ParseFromString(msg),\n",
        "\n",
        "obtÃ©m sd.image_data[0].image.raw_data (bytes JPEG do frame),\n",
        "\n",
        "decodifica para numpy com cv2.imdecode(...).\n",
        "\n",
        "guarda ~150 frames e salva um vÃ­deo .mp4 (15 fps) com cv2.VideoWriter.\n",
        "\n",
        "-- + robusto:\n",
        "\n",
        "LÃª cada mensagem protobuf do .osi (cÃ¢mera),\n",
        "\n",
        "Percorre recursivamente os campos e submensagens,\n",
        "\n",
        "Detecta bytes com â€œassinaturaâ€ de JPEG (FF D8 â€¦ FF D9) ou PNG (89 50 4E 47),\n",
        "\n",
        "Decodifica com OpenCV e gera o preview MP4.\n",
        "\n",
        "-- SoluÃ§Ã£o do erro:\n",
        "Esse arquivo Ã© camera_sv_â€¦ (o â€œsvâ€ costuma indicar SensorView, nÃ£o SensorData).\n",
        "\n",
        "Em TWICE, cÃ¢mera/LiDAR foram â€œempacotadosâ€ como mensagens individuais dentro do .osi (loop [len][protobuf]), e algumas builds do OSI mudam os nomes/caminhos dos campos.\n",
        "\n",
        "Para resolver:\n",
        "\n",
        "Tentar SensorData, depois SensorView (duas classes diferentes do OSI).\n",
        "\n",
        "Se ainda nÃ£o achar, varrer o blob inteiro procurando JPEG/PNG diretamente (SOI/EOI), e decodificar.\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: vocÃª reconstrÃ³i as imagens da cÃ¢mera a partir do arquivo OSI do TWICE (cada mensagem Ã© um frame), e cria um preview para inspecionar rapidamente se o caso/condiÃ§Ã£o climÃ¡tica estÃ¡ ok."
      ],
      "metadata": {
        "id": "SePp97THhSxh"
      },
      "id": "SePp97THhSxh"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6459d5c7",
      "metadata": {
        "id": "6459d5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b7f276-6892-4d6e-9f4b-a2561d18de9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¥ CÃ¢mera .osi: /content/TWICE/TWICEsubset/Scenarios/dynamic_ego/CCRb/real/daytime/cluster/test_run_1/Camera/camera_sv_350_300.osi\n",
            "âœ… Preview salvo em: /content/camera_preview.mp4  (151 frames)\n"
          ]
        }
      ],
      "source": [
        "# CEL 6 â€” iterar frames do .osi (SensorData â†’ SensorView â†’ varredura do blob) e salvar um MP4\n",
        "import cv2, struct\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Tentar importar classes OSI\n",
        "SensorData = None\n",
        "SensorView = None\n",
        "try:\n",
        "    from cuboid_project import SensorData as TW_SensorData\n",
        "    SensorData = TW_SensorData\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    # alguns pacotes expÃµem diretamente\n",
        "    import osi3\n",
        "    if SensorData is None and hasattr(osi3, \"SensorData\"):\n",
        "        SensorData = osi3.SensorData\n",
        "    # SensorView pode vir como atributo ou via mÃ³dulo pb2\n",
        "    if hasattr(osi3, \"SensorView\"):\n",
        "        SensorView = osi3.SensorView\n",
        "    else:\n",
        "        try:\n",
        "            from osi3 import sensorview_pb2\n",
        "            SensorView = sensorview_pb2.SensorView\n",
        "        except:\n",
        "            pass\n",
        "except:\n",
        "    pass\n",
        "\n",
        "JPEG_SOI = b\"\\xFF\\xD8\"\n",
        "JPEG_EOI = b\"\\xFF\\xD9\"\n",
        "PNG_SIG  = b\"\\x89PNG\\r\\n\\x1a\\n\"\n",
        "\n",
        "def looks_like_img(buf: bytes) -> bool:\n",
        "    return (\n",
        "        (len(buf) > 8 and buf.startswith(JPEG_SOI) and buf.endswith(JPEG_EOI)) or\n",
        "        (len(buf) > 8 and buf.startswith(PNG_SIG)) or\n",
        "        (len(buf) > 8 and buf.startswith(JPEG_SOI) and b\"\\xFF\\xD9\" in buf[-4:])\n",
        "    )\n",
        "\n",
        "def scan_blob_for_image(blob: bytes):\n",
        "    # Procura um JPEG (SOI...EOI)\n",
        "    start = blob.find(JPEG_SOI)\n",
        "    if start != -1:\n",
        "        end = blob.find(JPEG_EOI, start+2)\n",
        "        if end != -1:\n",
        "            return blob[start:end+2]\n",
        "    # Procura um PNG\n",
        "    start = blob.find(PNG_SIG)\n",
        "    if start != -1:\n",
        "        # PNG Ã© chunked; tentamos atÃ© o fim do blob\n",
        "        return blob[start:]\n",
        "    return None\n",
        "\n",
        "def extract_img_from_any_message(msg):\n",
        "    # Percorre recursivamente a mensagem protobuf atrÃ¡s de campos bytes que pareÃ§am imagem\n",
        "    try:\n",
        "        fields = msg.ListFields()\n",
        "    except Exception:\n",
        "        return None\n",
        "    from google.protobuf.descriptor import FieldDescriptor\n",
        "    for desc, val in fields:\n",
        "        try:\n",
        "            if desc.type == FieldDescriptor.TYPE_BYTES and looks_like_img(val):\n",
        "                return val\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Submensagem\n",
        "        if hasattr(val, \"ListFields\"):\n",
        "            out = extract_img_from_any_message(val)\n",
        "            if out is not None:\n",
        "                return out\n",
        "        # Lista de submensagens\n",
        "        if isinstance(val, (list, tuple)):\n",
        "            for elem in val:\n",
        "                if hasattr(elem, \"ListFields\"):\n",
        "                    out = extract_img_from_any_message(elem)\n",
        "                    if out is not None:\n",
        "                        return out\n",
        "    return None\n",
        "\n",
        "def iter_camera_frames(osi_path: Path):\n",
        "    with open(osi_path, \"rb\") as f:\n",
        "        while True:\n",
        "            len_bytes = f.read(4)\n",
        "            if not len_bytes:\n",
        "                break\n",
        "            (msg_len,) = struct.unpack(\"I\", len_bytes)\n",
        "            blob = f.read(msg_len)\n",
        "\n",
        "            # a) tentar SensorData\n",
        "            if SensorData is not None:\n",
        "                try:\n",
        "                    sd = SensorData()\n",
        "                    sd.ParseFromString(blob)\n",
        "                    img_bytes = extract_img_from_any_message(sd)\n",
        "                    if img_bytes:\n",
        "                        img_np = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "                        frame = cv2.imdecode(img_np, cv2.IMREAD_COLOR)\n",
        "                        if frame is not None:\n",
        "                            yield frame\n",
        "                            continue\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            # b) tentar SensorView\n",
        "            if SensorView is not None:\n",
        "                try:\n",
        "                    sv = SensorView()\n",
        "                    sv.ParseFromString(blob)\n",
        "                    img_bytes = extract_img_from_any_message(sv)\n",
        "                    if img_bytes:\n",
        "                        img_np = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "                        frame = cv2.imdecode(img_np, cv2.IMREAD_COLOR)\n",
        "                        if frame is not None:\n",
        "                            yield frame\n",
        "                            continue\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            # c) fallback bruto: vasculhar o blob por JPEG/PNG\n",
        "            img_bytes = scan_blob_for_image(blob)\n",
        "            if img_bytes:\n",
        "                img_np = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "                frame = cv2.imdecode(img_np, cv2.IMREAD_COLOR)\n",
        "                if frame is not None:\n",
        "                    yield frame\n",
        "                    continue\n",
        "            # Se chegou aqui, essa mensagem nÃ£o tinha imagem (ou formato nÃ£o suportado). Continua.\n",
        "\n",
        "# Localizar o arquivo de CÃ¢mera (vocÃª jÃ¡ tinha cam_file)\n",
        "scenario_dir = next(DATA_ROOT.rglob(\"Camera\"), None)\n",
        "assert scenario_dir is not None, \"NÃ£o encontrei /Camera no subset. Verifique a extraÃ§Ã£o do ZIP.\"\n",
        "cam_file = next(Path(scenario_dir).glob(\"*.osi\"))\n",
        "print(\"ğŸ¥ CÃ¢mera .osi:\", cam_file)\n",
        "\n",
        "frames = []\n",
        "for i, fr in enumerate(iter_camera_frames(cam_file)):\n",
        "    frames.append(fr)\n",
        "    if i >= 150:\n",
        "        break\n",
        "\n",
        "if not frames:\n",
        "    raise RuntimeError(\"Ainda nÃ£o consegui extrair frames. Rode a cÃ©lula de DIAGNÃ“STICO para inspecionar os campos.\")\n",
        "\n",
        "h, w = frames[0].shape[:2]\n",
        "out_path = \"/content/camera_preview.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "vw = cv2.VideoWriter(out_path, fourcc, 15, (w, h))\n",
        "for fr in frames:\n",
        "    vw.write(fr)\n",
        "vw.release()\n",
        "print(f\"âœ… Preview salvo em: {out_path}  ({len(frames)} frames)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 7 â€” carregar OpenLABEL e preparar a projeÃ§Ã£o 3Dâ†’2D\n",
        "\n",
        "o que faz:\n",
        "\n",
        "abre o *open*label*camera*.json do cenÃ¡rio,\n",
        "\n",
        "extrai a matriz intrÃ­nseca K a partir de camera_matrix_3x4 â†’ remodela para (3Ã—4) e usa as 3Ã—3 primeiras colunas,\n",
        "\n",
        "(na cÃ©lula de exemplo) sÃ³ escreve um texto no frame; para desenhar as arestas do cubo vocÃª usa:\n",
        "\n",
        "os â€œcuboidsâ€ (posiÃ§Ã£o, dimensÃµes, orientaÃ§Ã£o) em openlabel[\"objects\"][...][\"object_data\"][\"cuboid\"][\"val\"],\n",
        "\n",
        "funÃ§Ãµes do cuboid_project.py para obter os 8 cantos 3D (cuboid_corners), aplicar rotaÃ§Ã£o/translaÃ§Ã£o (extrÃ­nsecas) e projetar com K (intrÃ­nsecas),\n",
        "\n",
        "desenhar arestas com cv2.line.\n",
        "\n",
        "-- SoluÃ§Ã£o: isso acontece porque alguns arquivos OpenLABEL do TWICE nÃ£o usam a chave intrinsics_pinhole (ela aparece no exemplo do artigo, mas no subset real pode estar com outro nome/caminho).\n",
        "\n",
        "CEL 7 substituta, mais robusta, que:\n",
        "\n",
        "carrega o JSON do OpenLABEL,\n",
        "\n",
        "nesse JSON do OpenLABEL as intrÃ­nsecas nÃ£o estÃ£o gravadas (sÃ³ hÃ¡ timestamp e Streams). Sem dramas: dÃ¡ pra seguir de duas formas:\n",
        "\n",
        "Procurar mais fundo dentro de Streams.Camera1 (Ã s vezes a matriz aparece com outro nome/caminho).\n",
        "\n",
        "Fallback fÃ­sico: estimar K a partir do FOV + tamanho da imagem. O artigo informa ~60Â° (horizontal) e ~36.6Â° (vertical) para a Sekonix 1920Ã—1232 â€” isso Ã© suficiente para projetar cubos com boa aproximaÃ§Ã£o.\n",
        "\n",
        "procura recursivamente por matrizes intrÃ­nsecas em formatos comuns:\n",
        "\n",
        "camera_matrix_3x4 (tira a parte 3Ã—3),\n",
        "\n",
        "K (3Ã—3),\n",
        "\n",
        "campos soltos fx, fy, cx, cy,\n",
        "\n",
        "outras variaÃ§Ãµes como camera_matrix, intrinsics, etc.\n",
        "\n",
        "imprime qual caminho foi encontrado,\n",
        "\n",
        "desenha um texto no primeiro frame (sÃ³ para confirmar; depois a gente pluga no desenho 3Dâ†’2D).\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: o TWICE rotula em OpenLABEL (padrÃ£o da ASAM): vocÃª aproveita essas anotaÃ§Ãµes para desenhar ground-truth sobre os frames, checar calibraÃ§Ã£o e gerar vÃ­deos anotados."
      ],
      "metadata": {
        "id": "bd-uyT-wiOS_"
      },
      "id": "bd-uyT-wiOS_"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "588ebaad",
      "metadata": {
        "id": "588ebaad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1cf40e-f894-4f1a-b2de-374e646e112a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§¾ OpenLABEL: /content/TWICE/TWICEsubset/Scenarios/dynamic_ego/CCRb/real/daytime/cluster/test_run_1/Camera/open_label_camera.json\n",
            "âœ… K usada: fallback FOV (60.0Â° Ã— 36.6Â°) + frame 1920Ã—1208\n",
            "[[1.66276878e+03 0.00000000e+00 9.60000000e+02]\n",
            " [0.00000000e+00 1.82632728e+03 6.04000000e+02]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
            "ğŸ–¼ï¸ Exemplo salvo em /content/cuboids_demo.jpg\n"
          ]
        }
      ],
      "source": [
        "# CEL 7 (robusta v2) â€” localizar K em Streams.Camera1 OU estimar por FOV (fallback)\n",
        "import json, cv2, numpy as np, math\n",
        "from pathlib import Path\n",
        "\n",
        "openlabel_file = next(Path(scenario_dir).glob(\"*open*label*camera*.json\"), None)\n",
        "print(\"ğŸ§¾ OpenLABEL:\", openlabel_file)\n",
        "\n",
        "with open(openlabel_file, \"r\") as f:\n",
        "    ol = json.load(f)\n",
        "\n",
        "# 1) pega o nÃ³ Camera1 (se existir)\n",
        "cam1 = {}\n",
        "try:\n",
        "    cam1 = ol[\"openlabel\"][\"frames\"][0][\"frame_properties\"][\"Streams\"][\"Camera1\"]\n",
        "except Exception:\n",
        "    cam1 = {}\n",
        "\n",
        "def _flatten(d, prefix=\"\"):\n",
        "    if isinstance(d, dict):\n",
        "        for k,v in d.items():\n",
        "            p = f\"{prefix}.{k}\" if prefix else k\n",
        "            yield from _flatten(v, p)\n",
        "    elif isinstance(d, list):\n",
        "        for i,v in enumerate(d):\n",
        "            p = f\"{prefix}[{i}]\"\n",
        "            yield from _flatten(v, p)\n",
        "    else:\n",
        "        yield prefix, d\n",
        "\n",
        "def _find_K_in_dict(node):\n",
        "    # procura 3x4 -> 3x3\n",
        "    for path, val in _flatten(node):\n",
        "        if isinstance(val, list) and len(val)==12 and all(isinstance(x,(int,float)) for x in val):\n",
        "            if any(tok in path.lower() for tok in [\"camera_matrix_3x4\",\"matrix_3x4\",\"3x4\",\"pinhole\"]):\n",
        "                K = np.array(val, dtype=float).reshape(3,4)[:,:3]\n",
        "                return K, f\"{path} (3x4â†’K)\"\n",
        "    # procura 3x3 direta\n",
        "    for path, val in _flatten(node):\n",
        "        if isinstance(val, list) and len(val)==9 and all(isinstance(x,(int,float)) for x in val):\n",
        "            if any(tok in path.lower() for tok in [\"camera_matrix\",\"intrinsics\",\"k\",\"matrix_3x3\",\"3x3\"]):\n",
        "                K = np.array(val, dtype=float).reshape(3,3)\n",
        "                return K, f\"{path} (3x3)\"\n",
        "    # fx,fy,cx,cy no mesmo nÃ­vel\n",
        "    def _fxfy_from_dict(d):\n",
        "        if not isinstance(d, dict): return None\n",
        "        keys = {k.lower():k for k in d.keys()}\n",
        "        need = [\"fx\",\"fy\",\"cx\",\"cy\"]\n",
        "        if all(n in keys for n in need):\n",
        "            try:\n",
        "                fx = float(d[keys[\"fx\"]]); fy = float(d[keys[\"fy\"]])\n",
        "                cx = float(d[keys[\"cx\"]]); cy = float(d[keys[\"cy\"]])\n",
        "                K  = np.array([[fx,0,cx],[0,fy,cy],[0,0,1]],dtype=float)\n",
        "                return K\n",
        "            except: return None\n",
        "        return None\n",
        "    def _walk(d, path=\"\"):\n",
        "        if isinstance(d, dict):\n",
        "            K = _fxfy_from_dict(d)\n",
        "            if K is not None:\n",
        "                return K, f\"{path or 'fx/fy/cx/cy'}\"\n",
        "            for k,v in d.items():\n",
        "                out = _walk(v, f\"{path}.{k}\" if path else k)\n",
        "                if out: return out\n",
        "        elif isinstance(d, list):\n",
        "            for i,v in enumerate(d):\n",
        "                out = _walk(v, f\"{path}[{i}]\")\n",
        "                if out: return out\n",
        "        return None\n",
        "    out = _walk(node)\n",
        "    if out: return out\n",
        "    return None, None\n",
        "\n",
        "K, where = _find_K_in_dict(cam1)\n",
        "\n",
        "if K is None:\n",
        "    # 2) Fallback: estimar K por FOV (do paper) + tamanho do frame\n",
        "    # pegue dimensÃµes do frame jÃ¡ decodificado na CEL 6\n",
        "    H, W = frames[0].shape[:2]\n",
        "    # Tenta ler FOV do JSON (se existir); caso contrÃ¡rio, usa nominal 60Â° Ã— 36.6Â°\n",
        "    hfov_deg = 60.0\n",
        "    vfov_deg = 36.6\n",
        "    # procure possÃ­veis campos de FOV:\n",
        "    for path,val in _flatten(cam1):\n",
        "        if isinstance(val,(int,float)) and \"fov\" in path.lower():\n",
        "            # heurÃ­stica: se encontrar dois floats, assume primeiro horizontal e segundo vertical\n",
        "            if \"horizontal\" in path.lower(): hfov_deg = float(val)\n",
        "            if \"vertical\"   in path.lower(): vfov_deg = float(val)\n",
        "    # converte para radianos\n",
        "    hfov = math.radians(hfov_deg)\n",
        "    vfov = math.radians(vfov_deg)\n",
        "    # modelo pinhole: fx = (W/2)/tan(hfov/2); fy = (H/2)/tan(vfov/2)\n",
        "    fx = (W/2.0) / math.tan(hfov/2.0)\n",
        "    fy = (H/2.0) / math.tan(vfov/2.0)\n",
        "    cx = W/2.0\n",
        "    cy = H/2.0\n",
        "    K = np.array([[fx,0,cx],[0,fy,cy],[0,0,1]], dtype=float)\n",
        "    where = f\"fallback FOV ({hfov_deg:.1f}Â° Ã— {vfov_deg:.1f}Â°) + frame {W}Ã—{H}\"\n",
        "\n",
        "print(\"âœ… K usada:\", where)\n",
        "print(K)\n",
        "\n",
        "# MarcaÃ§Ã£o simples no primeiro frame sÃ³ para confirmar\n",
        "frame0 = frames[0].copy()\n",
        "cv2.putText(frame0, f\"K: {where}\", (30,40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
        "cv2.imwrite(\"/content/cuboids_demo.jpg\", frame0)\n",
        "print(\"ğŸ–¼ï¸ Exemplo salvo em /content/cuboids_demo.jpg\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 8 â€” visualizar radar (cluster / object list) e trajetÃ³rias\n",
        "\n",
        "o que faz: importa radar_osi_plot.py, localiza um .osi de radar (se existir no subset daquele cenÃ¡rio) e orienta usar as funÃ§Ãµes do script para plotar:\n",
        "\n",
        "cluster: nuvem de pontos (cada reflexÃ£o do radar),\n",
        "\n",
        "object list: lista de objetos agregados pelo ECU (com velocidade/posiÃ§Ã£o/RCS),\n",
        "\n",
        "posiÃ§Ã£o do ego e do alvo (IMU/GNSS) ao longo do tempo.\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: permite ver como o radar â€œenxergaâ€ o mesmo cenÃ¡rio da cÃ¢mera (real e HIL), inclusive comparando modos de operaÃ§Ã£o."
      ],
      "metadata": {
        "id": "qqHXrOwhi63l"
      },
      "id": "qqHXrOwhi63l"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1531b258",
      "metadata": {
        "id": "1531b258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611cfc7a-ef3b-44af-e538-4999c057926b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¡ Radar .osi: /content/TWICE/TWICEsubset/Scenarios/dynamic_ego/CCRb/real/daytime/cluster/test_run_1/Radar/radar_sd_350_300.osi\n"
          ]
        }
      ],
      "source": [
        "# CEL 8 â€” visualizar radar (cluster/object list) â€” precisa de arquivo em /Radar\n",
        "from importlib import reload\n",
        "import os\n",
        "try:\n",
        "    import radar_osi_plot as rplt\n",
        "    reload(rplt)\n",
        "    radar_dir = (Path(scenario_dir).parent / \"Radar\")\n",
        "    if radar_dir.exists():\n",
        "        radar_osi = next(radar_dir.glob(\"*.osi\"))\n",
        "        print(\"ğŸ“¡ Radar .osi:\", radar_osi)\n",
        "        # Abra o arquivo radar_osi_plot.py para ver como salvar vÃ­deo (hÃ¡ exemplo lÃ¡).\n",
        "    else:\n",
        "        print(\"âš ï¸ Este caso do subset pode nÃ£o ter Radar. Teste outro cenÃ¡rio ou use o dataset completo.\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ Import do script de radar falhou:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEL 9 â€” (opcional) rodar YOLOv7 como demo\n",
        "\n",
        "o que faz: exporta ~30 frames do preview para /content/frames/*.jpg, clona o repositÃ³rio do YOLOv7 e roda detect.py com yolov7.pt. salva imagens e labels preditos em /content/y7out/det.\n",
        "\n",
        "instala PyTorch jÃ¡ compilado (CUDA 12.1, CPU fallback), clona o YOLOv7 e roda o detect.py.\n",
        "\n",
        "relaÃ§Ã£o com o TWICE: demonstra um baseline de detecÃ§Ã£o sobre os frames do TWICE. a partir daÃ­, vocÃª pode comparar com GT do OpenLABEL (convertendo cuboids 3D â†’ caixas 2D) para calcular IoU/AP, como o artigo exemplifica."
      ],
      "metadata": {
        "id": "jOW9TCajjTbp"
      },
      "id": "jOW9TCajjTbp"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1b928cb8",
      "metadata": {
        "id": "1b928cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e009c4d-ba54-4e68-fb71-b95f5f670922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "â¬‡ï¸ Instalando PyTorch + TorchVision (CUDA 12.1 wheels)...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m707.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m/content/yolov7\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy<1.24.0,>=1.18.5 (from versions: 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy<1.24.0,>=1.18.5\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "/content\n",
            "/content/yolov7\n",
            "Namespace(weights=['yolov7.pt'], source='/content/frames', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=True, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='/content/y7out', name='det', exist_ok=True, no_trace=False)\n",
            "YOLOR ğŸš€ v0.1-128-ga207844 torch 2.4.1+cu121 CPU\n",
            "\n",
            "Downloading https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt to yolov7.pt...\n",
            "100% 72.1M/72.1M [00:00<00:00, 131MB/s]\n",
            "\n",
            "/content/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(w, map_location=map_location)  # load\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "Model Summary: 306 layers, 36905341 parameters, 6652669 gradients\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "1 person, 1 car, Done. (1075.0ms) Inference, (1.0ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0000.jpg\n",
            "1 car, Done. (1001.8ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0001.jpg\n",
            "1 car, Done. (1009.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0002.jpg\n",
            "1 car, Done. (1005.0ms) Inference, (1.1ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0003.jpg\n",
            "1 person, 1 car, Done. (1250.5ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0004.jpg\n",
            "1 car, Done. (1002.1ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0005.jpg\n",
            "1 car, Done. (1009.1ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0006.jpg\n",
            "1 car, Done. (999.9ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0007.jpg\n",
            "1 car, Done. (991.9ms) Inference, (0.6ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0008.jpg\n",
            "1 person, 1 car, 1 truck, Done. (1008.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0009.jpg\n",
            "1 car, Done. (1012.4ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0010.jpg\n",
            "1 car, Done. (1003.7ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0011.jpg\n",
            "1 car, Done. (1004.5ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0012.jpg\n",
            "1 car, Done. (996.2ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0013.jpg\n",
            "1 car, Done. (1091.2ms) Inference, (0.9ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0014.jpg\n",
            "1 car, Done. (1165.8ms) Inference, (0.6ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0015.jpg\n",
            "1 car, 1 truck, Done. (990.7ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0016.jpg\n",
            "1 car, Done. (992.7ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0017.jpg\n",
            "1 car, Done. (999.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0018.jpg\n",
            "1 car, Done. (997.5ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0019.jpg\n",
            "1 car, Done. (1036.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0020.jpg\n",
            "1 car, Done. (996.9ms) Inference, (0.6ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0021.jpg\n",
            "1 car, Done. (998.9ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0022.jpg\n",
            "1 car, Done. (992.0ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0023.jpg\n",
            "1 car, Done. (993.5ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0024.jpg\n",
            "1 car, Done. (1159.3ms) Inference, (0.9ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0025.jpg\n",
            "1 car, Done. (1116.6ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0026.jpg\n",
            "1 car, Done. (999.4ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0027.jpg\n",
            "1 car, Done. (1016.4ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0028.jpg\n",
            "1 car, Done. (1013.6ms) Inference, (0.7ms) NMS\n",
            " The image with the result is saved in: /content/y7out/det/0029.jpg\n",
            "Done. (31.547s)\n",
            "âœ… Resultados em:\n",
            "  - imagens: /content/y7out/det\n",
            "  - labels:  /content/y7out/det/labels\n"
          ]
        }
      ],
      "source": [
        "# === YOLOv7 - instalaÃ§Ã£o robusta e inferÃªncia nos frames ===\n",
        "import os, cv2, pathlib, sys, subprocess, textwrap\n",
        "\n",
        "# 0) Garantir que estamos na raiz do Colab\n",
        "%cd /content\n",
        "\n",
        "# 1) Clonar repositÃ³rio yolov7 (se ainda nÃ£o existe)\n",
        "if not pathlib.Path(\"/content/yolov7\").exists():\n",
        "    !git clone -q https://github.com/WongKinYiu/yolov7.git\n",
        "else:\n",
        "    print(\"ğŸ” yolov7 jÃ¡ clonado.\")\n",
        "\n",
        "# 2) Instalar PyTorch prÃ©-compilado (tenta CUDA 12.1; se falhar, cai para CPU)\n",
        "def install_torch():\n",
        "    try:\n",
        "        print(\"â¬‡ï¸ Instalando PyTorch + TorchVision (CUDA 12.1 wheels)...\")\n",
        "        !pip -q install --upgrade \"torch>=2.2,<3\" \"torchvision>=0.17,<0.20\" --index-url https://download.pytorch.org/whl/cu121\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Falhou CUDA 12.1, tentando CPU wheelsâ€¦\")\n",
        "        !pip -q install --upgrade \"torch>=2.2,<3\" \"torchvision>=0.17,<0.20\" --index-url https://download.pytorch.org/whl/cpu\n",
        "install_torch()\n",
        "\n",
        "# 3) Instalar dependÃªncias do YOLOv7 (ignorando torch, que jÃ¡ instalamos acima)\n",
        "%cd /content/yolov7\n",
        "# remove linhas de torch do requirements (se houver) para evitar conflitos\n",
        "!grep -viE 'torch|torchvision' requirements.txt > /content/yolov7/req_no_torch.txt\n",
        "# instalar com wheels quando possÃ­vel (evita compilar)\n",
        "!pip -q install -r req_no_torch.txt --only-binary=:all: || pip -q install -r req_no_torch.txt\n",
        "\n",
        "# 4) Preparar frames (caso ainda nÃ£o tenha sido feito)\n",
        "%cd /content\n",
        "import os, cv2\n",
        "os.makedirs(\"/content/frames\", exist_ok=True)\n",
        "if len(os.listdir(\"/content/frames\")) == 0:\n",
        "    # usa os frames coletados na CEL 6 (lista 'frames') para exportar ~30 imagens\n",
        "    for i, fr in enumerate(frames[:30]):\n",
        "        cv2.imwrite(f\"/content/frames/{i:04d}.jpg\", fr)\n",
        "    print(\"ğŸ–¼ï¸ Exportei\", min(30, len(frames)), \"frames para /content/frames\")\n",
        "\n",
        "# 5) Rodar detecÃ§Ã£o\n",
        "%cd /content/yolov7\n",
        "!python detect.py --weights yolov7.pt --conf 0.25 --source /content/frames --save-txt --project /content/y7out --name det --exist-ok\n",
        "\n",
        "print(\"âœ… Resultados em:\")\n",
        "print(\"  - imagens: /content/y7out/det\")\n",
        "print(\"  - labels:  /content/y7out/det/labels\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}